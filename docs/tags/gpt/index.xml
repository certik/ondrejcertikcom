<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GPT on Ondřej Čertík</title>
    <link>https://ondrejcertik.com/tags/gpt/</link>
    <description>Recent content in GPT on Ondřej Čertík</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Mar 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ondrejcertik.com/tags/gpt/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>fastGPT: Faster than PyTorch in 300 lines of Fortran</title>
      <link>https://ondrejcertik.com/blog/2023/03/fastgpt-faster-than-pytorch-in-300-lines-of-fortran/</link>
      <pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ondrejcertik.com/blog/2023/03/fastgpt-faster-than-pytorch-in-300-lines-of-fortran/</guid>
      <description>Authors: Ondřej Čertík, Brian Beckman
In this blog post I am announcing fastGPT, fast GPT-2 inference written in Fortran. In it, I show
  Fortran has speed at least as good as default PyTorch on Apple M1 Max.
  Fortran code has statically typed arrays, making maintenance of the code easier than with Python
  It seems that the bottleneck algorithm in GPT-2 inference is matrix-matrix multiplication. For physicists like us, matrix-matrix multiplication is very familiar, unlike other aspects of AI and ML.</description>
    </item>
    
  </channel>
</rss>