[{"categories":null,"contents":"SymPy is a Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS) while keeping the code as simple as possible in order to be comprehensible and easily extensible.\nOndřej started the project in 2005, and since then developed a large user and developer community. He handed over the leadership to Aaron Meurer in 2011.\nSymPy has participated in every Google Summer of Code since 2007 and Ondřej has been either a mentor or an organization admin (together with Aaron Meurer). You can see here for full details.\n","permalink":"https://ondrejcertik.com/projects/contributions/sympy/","tags":["Computer Algebra System","Symbolic Manipulation","Python"],"title":"SymPy"},{"categories":null,"contents":"SymEngine is a fast symbolic manipulation library, written in C++.\nOndřej started the project in 2012, and since then developed a user and developer community. According to many benchmarks, SymEngine is one of the fastest computer algebra systems (whether open source or commercial).\nSymEngine is written in pure C++, but has wrappers to many languages (Python, Julia, Ruby, \u0026hellip;). It can optionally be used as a backend in SymPy.\n","permalink":"https://ondrejcertik.com/projects/contributions/symengine/","tags":["Computer Algebra System","Symbolic Manipulation","C++","Performance"],"title":"SymEngine"},{"categories":null,"contents":"LFortran is a modern open-source (BSD licensed) interactive Fortran compiler built on top of LLVM. It can execute user\u0026rsquo;s code interactively to allow exploratory work (much like Python, MATLAB or Julia) as well as compile to binaries with the goal to run user\u0026rsquo;s code on modern architectures such as multi-core CPUs and GPUs.\nOndřej started the project in 2017, and is working towards getting first users and developers. He plans to make it a successful open source projects just like SymPy or SymEngine.\n","permalink":"https://ondrejcertik.com/projects/contributions/lfortran/","tags":["Compilers","Fortran","Interactive","Jupyter","Performance"],"title":"LFortran"},{"categories":null,"contents":"The Theoretical Physics Reference is an attempt to derive all theoretical physics equations (that are ever needed for applications) from the general and special relativity and the standard model of particle physics.\nThe goals are:\nAll calculations are very explicit, with no intermediate steps left out. Start from the most general (and correct) physical theories (general relativity or standard model) and derive the specialized equations from them (e.g. the Schrödinger equation). Math is developed in the math section (not in the physics section). Theory should be presented as short and as explicitly as possible. Then there should be arbitrary number of examples, to show how the theory is used. There should be just one notation used throughout the book. It should serve as a reference to any physics equation (exact derivation where it comes from) and the reader should be able to understand how things work from this book, and be ready to understand specialized literature. This is a work in progress and some chapters don’t conform to the above goals yet. Usually first some derivation is written, as we understood it, then the mathematical tools are extracted and put into the math section, and the rest is fit where it belongs. Sometimes we don’t understand some parts yet, then those are currently left there as they are.\nAt the page you can find html version of the book, source code, pdf and also order a physical hardcover book.\n","permalink":"https://ondrejcertik.com/projects/contributions/tfr/","tags":["Theoretical Physics","Book","Reference"],"title":"Theoretical Physics Reference"},{"categories":null,"contents":" Fortran 90 is a website which lists recommended practices for modern Fortran. It also has a section about side by side comparison with Python and an FAQ about Fortran.\n","permalink":"https://ondrejcertik.com/projects/contributions/fortran90/","tags":["Fortran","Best Practices","Python","Rosetta Stone"],"title":"Fortran 90 Webpage"},{"categories":null,"contents":"Authors: Ondřej Čertík, Brian Beckman\nIn this blog post I am announcing fastGPT, fast GPT-2 inference written in Fortran. In it, I show\nFortran has speed at least as good as default PyTorch on Apple M1 Max.\nFortran code has statically typed arrays, making maintenance of the code easier than with Python\nIt seems that the bottleneck algorithm in GPT-2 inference is matrix-matrix multiplication. For physicists like us, matrix-matrix multiplication is very familiar, unlike other aspects of AI and ML. Finding this familiar ground inspired us to approach GPT-2 like any other numerical computing problem.\nFixed an unintentional single-to-double conversion that slowed down the original Python.\nI am asking others to take over and parallelize fastGPT on CPU and offload to GPU and see how fast you can make it.\nAbout one month ago, I read the blogpost GPT in 60 Lines of NumPy, and it piqued my curiosity. I looked at the corresponding code (picoGPT) and was absolutely amazed, for two reasons. First, I hadn\u0026rsquo;t known it could be so simple to implement the GPT-2 inference. Second, this looks just like a typical computational physics code, similar to many that I have developed and maintained throughout my career.\nI immediately downloaded picoGPT to test it out and indeed it worked! It was slow, as advertised, but it worked and it gave exactly the same answer as PyTorch. Then I studied the source code more and indeed it seemed like a clean, full, self-contained implementation of GPT-2.\nThe next step is obvious: this is just a numerical array-oriented algorithm, so if we want it to look like NumPy, but to be fast like PyTorch, let\u0026rsquo;s rewrite in Fortran!\nFollowing picoGPT as a reference, I straightforwardly rewrote one function at a time to Fortran, and checked against picoGPT that my Fortran gives exactly the same answer. The job took about two afternoons. Both picoGPT and PyTorch (from conda-forge) use OpenBLAS to run in parallel on Apple M1, so I linked my Fortran against OpenBLAS also to get fast matrix-matrix multiplies. Without any other optimizations, my Fortran gave faster inference than PyTorch!\nWhile writing picoGPT into fastGPT, I noticed that picoGPT accidentally casts the computation from single to double precision. I sent a PR to picoGPT that fixes that, speeding it up 5x for me. I use the faster version below.\nI also implemented kv-cache, which greatly speeds up token generation beyond the first version of fastGPT. Below, \u0026ldquo;no cache\u0026rdquo; means kv-cache is turned off. Let\u0026rsquo;s look at the benchmarks on my laptop. On Apple M1 Max we do the GPT-2 124M model inference of 19 input tokens and generating 20 more tokens (see the README for more details). The following two lines are the most fair comparison against PyTorch: just the inference itself, excluding all initialization; using the same backend (OpenBLAS); using caching (the default in PyTorch); all compiler optimizations on, but no special-purpose code in fastGPT. In our opinion we give the maximum possible advantage to PyTorch and we are faster on all cores (1-8):\nCode 1 core 2 cores 4 cores 8 cores fastGPT (OpenBLAS) 0.837s 0.514s 0.341s 0.339s PyTorch (OpenBLAS) 0.873s 0.539s 0.386s 0.392s In the second table we now introduce two improvements: faster implementation of the tanh function and using the Accelerate framework on macOS, now the results are 3x faster on single core.\nCode 1 core 2 cores 4 cores 8 cores fastGPT (Accelerate, fast_tanh) 0.288s fastGPT (Accelerate) 0.299s fastGPT (OpenBLAS) 0.837s 0.514s 0.341s 0.339s PyTorch (OpenBLAS) 0.873s 0.539s 0.386s 0.392s In the third table we also compare against picoGPT, which does not have caching implemented, so we turn off caching in fastGPT and PyTorch and again use the same backend (OpenBLAS) and no special optimizations in fastGPT, for fair comparison:\nCode 1 core 2 cores 4 cores 8 cores fastGPT (OpenBLAS, no cache) 2.343s 1.603s 1.209s 1.018s PyTorch (OpenBLAS, no cache) 2.356s 1.520s 1.104s 0.997s picoGPT (OpenBLAS, no cache) 2.427s 1.645s 1.272s 1.081s The above benchmarks only compare the time for the inference itself, excluding loading the data (for all codes) and Python import times (for picoGPT and PyTorch). With IO optimized for Fortran arrays, the results are truly dramatic, up to 12x faster. Total run (includes loading the model and Python imports):\nCode Time fastGPT (Accelerate, fast_tanh) 0.401s picoGPT (8 cores) 3.445s PyTorch (OpenBLAS, 4 cores) 4.867s As you can see, fastGPT is slightly faster than PyTorch when doing as fair comparison as we can (both using OpenBLAS as a backend and both using caching, the default in PyTorch). You can also see that fastGPT loads the model very quickly and runs immediately, while both PyTorch and picoGPT take a long time to both load the model and to import all the Python libraries.\nThis matches my past experience with Fortran. Every time I rewrite NumPy code in Fortran, it looks almost the same, but I get very competitive performance. Until now I have not been interested in machine learning / AI, because it seemed to me like very large fits to data, plus the results were not even very impressive to me, and the algorithms themselves did not seem similar to computational physics. But GPT-2, after implementing a Fortran version of it, I can say without any doubt that the algorithm is exactly analogous to many computational physics codes that I have been working with. Consequently, I think exactly the same performance techniques apply here.\nUsing a language like Fortran, which is oriented to the fastest possible array computations, allows to write code that is the highly performing, but still readable, because things get complicated and one must be able to maintain it. (The GPT-2 inference algorithm is actually quite simple compared to most physics codes.)\nBoth maintainability and speed is achieved by array declarations with static types, compare the original Python:\ndef mha(x, c_attn, c_proj, n_head): # [n_seq, n_embd] -\u0026gt; [n_seq, n_embd] ... and Fortran:\nfunction mha(n_seq, n_embd, x, attn_w, attn_b, proj_w, proj_b, n_head) result(y) integer, intent(in) :: n_seq, n_embd, n_head real(sp), intent(in) :: x(n_embd,n_seq), \u0026amp; attn_w(3*n_embd,n_embd), attn_b(3*n_embd), \u0026amp; proj_w(n_embd,n_embd), proj_b(n_embd) real(sp) :: y(n_embd,n_seq) ... In picoGPT one must use comments to keep track of the dimensions, and sometimes there are mistakes, which is inevitable. In Fortran the compiler itself ensures all the dimensions are correct with compile and runtime checks. It is great for both documentation and speed. The Python version actually accepts c_attn which is a dictionary of arrays. For performance I do not recommend that, so we pass all the underlying arrays directly. Besides these declarations, the Fortran code is almost identical to the original NumPy code.\nIf you like these results so far, please help us parallelize fastGPT on CPU as well as offload to GPU. We have a very good single core CPU performance (but we should still try to speed it up further), and it provides a great foundation for parallelization. Let\u0026rsquo;s see how fast we can make it!\nDiscussions:\nTwitter: https://twitter.com/OndrejCertik/status/1635768419307110400 Fortran Discourse: https://fortran-lang.discourse.group/t/fastgpt-faster-than-pytorch-in-300-lines-of-fortran/ Hacker News: https://news.ycombinator.com/item?id=35159961 ","permalink":"https://ondrejcertik.com/blog/2023/03/fastgpt-faster-than-pytorch-in-300-lines-of-fortran/","tags":["Fortran","GPT","Performance"],"title":"fastGPT: Faster than PyTorch in 300 lines of Fortran"},{"categories":null,"contents":"In December I left Los Alamos National Laboratory (LANL) to join GSI Technology as a compiler developer.\nI have spent about 8.5 years at LANL. Physics and computing have always been my passions, so I got a Master in theoretical physics in Czechia, and then Ph.D. in chemical physics in the U.S. LANL is a great place for computational physics. I met and worked with many amazing people. It has been intellectually very satisfying to work on the projects that I was able to help with and things to learn. LANL has famous beginnings during the Manhattan Project, and it has continuously evolved since then. For somebody who was born behind the Iron Curtain, it\u0026rsquo;s been an honor to work there. If you like physics or scientific computing, I can only recommend applying there.\nThe last few years I have been working very hard to create a modern Fortran compiler called LFortran. The way we have designed it is so that you can target the intermediate representation, that we call the Abstract Semantic Representation (ASR), from other languages also. It is a compiler toolchain that any array language can target. I got an opportunity to join GSI Technology to focus full time to work on this toolchain, to make their APU chips easier to program. We are also developing a new Python frontend called LPython. Both LFortran and LPython are effectively thin frontends that do parsing to a language-specific Abstract Syntax Tree (AST) and then transform AST to a language-independent ASR by applying and checking the surface-language semantics.\nBoth LPython and LFortran compilers will remain independent open-source projects with no particular specialization to any particular array chip. I am joining a very good compiler team, and it is an opportunity to deliver on this broader compiler project as well as LFortran and LPython in particular.\nIf you are interested in collaborating on any of these projects, please let me know. We have intern and other opportunities available.\n","permalink":"https://ondrejcertik.com/blog/2022/04/leaving-los-alamos-national-laboratory-and-joining-gsi-technology/","tags":["Job","Compilers","Python","Fortran"],"title":"Leaving Los Alamos National Laboratory and joining GSI Technology"},{"categories":null,"contents":"We have finished the first chapter on a long journey of resurrecting Fortran: forming an initial core community of developers and users with enough momentum around the new Fortran website and projects at fortran-lang.org. There is still have a long way to go to, but this was a necessary (although not sufficient) achievement and I feel this was a once in a life time event. The timing was right, we were ready, and we also got lucky. In this blog post I have tried to write up the details about the pivotal moments from my perspective. I have asked Milan Curcic who has been a co-founder of this effort to do the same (please see his blog post), more on that below. Brad Richardson has also written about his perspective.\nThis has been the most unusual experience for me, nothing like what I anticipated, but two years ago things seemed to fall into place and set up just right. I had to give it everything I had in order to give it a proper chance. I took the bull by the horns.\nThis blog post is long, but you might find it useful to get ideas how to form other such communities.\nBackground I have been using Fortran on and off for a long time, but only started using it as my main language around 2010 thanks to John Pask, my advisor at LLNL at a time, who introduced me to modern Fortran and I saw that it was a superior choice for high performance numerical code than Python - my main choice until that time. I immediately realized there is not much information about modern Fortran online, so I started the fortran90.org website in 2012 to collect information about the recommended modern ways to use Fortran with the hope that it could become the website for Fortran. Over the years it received contributions from about 11 people and it is moderately popular, but overall it failed to create a well agreed upon website and a community for Fortran. fortran-utils, was another idea, which got 5 contributors and 142 stars at GitHub over the years. Not bad, but no way close to creating a thriving community. I thought perhaps there just wasn\u0026rsquo;t enough interest in Fortran, just a few enthusiasts here and there, not enough to build a community.\nLFortran Fast forward to late 2017. I got a thought: how could Fortran be interactive as Python? I thought a lot of the issues with Fortran could be fixed with a better, interactive compiler. I got to work. This eventually became LFortran, but for the time, I worked on it privately.\nAfter a year of work, I decided to join the Fortran Standards Committee, which I didn\u0026rsquo;t know much about at the time. I knew it existed, and I wanted to know if it is still active, and if we can join forces somehow. The first meeting I was able to attend was in February 2019. I showed LFortran to the committee and there was a lot of interest. That prompted me to get it to a point where it could be announced publicly. This event occurred in April 2019 (which was exciting for me), you can read the announcement blog post. It generated tremendous interest (over 3000 people read it in the first few days). It became obvious that people wanted to hear more about why we thought Fortran was a great choice. We explained that in a follow up blog post.\nThis interest in Fortran and LFortran was unusual. The response to LFortran lead me to believe I was creating something useful. Almost anybody I talked to, even if they were not fans of Fortran initially, were interested after hearing about the project. I showed individuals how LFortran works, and how it is interactive and works in a Jupyter notebook just like Python or Julia. I could feel that opinions about Fortran are slowly changing. I was hoping that we could build a community around LFortran to resurrect Fortran. I have communicated with dozens and dozens of people, online and offline, publicly and privately. But so far people were still waiting, keeping an eye on LFortran, but not ready to join the effort yet.\nIncubator Repository My next Fortran Standards Committee meeting was in October 2019. Zach Jibben joined me as another representative for LANL. The meeting started on Monday. On Tuesday night I was in my hotel room looking from a window at Las Vegas at night, and I got the idea that we should track the committee proposals. First I thought to just open issues at LFortran\u0026rsquo;s GitLab repository. I called Zach and asked his opinion on the idea. We decided to dedicate a repository to it, so I created a fortran_proposals repository at LFortran\u0026rsquo;s GitHub organization (I chose GitHub to get more participation, as it has more users than GitLab). We started adding issues for ideas that would be worth discussing to include in Fortran.\nIn order to draw more attention to the idea, I posted to ask on Twitter \u0026ldquo;I am at the committee meeting, if anyone has any wishes that I am happy to represent it\u0026rdquo;. There were several excellent suggestions, such as the very first one from Michael Zingale that eventually became the first issue #1. It was at this time that Milan got more involved.\nThe next day at the committee meeting, my main job was to sell the idea to the other committee members, so they could embrace the repository. Gary Klimowicz has been tremendously helpful convicing others. The first thing he said we needed was to move it to its own organization, so we moved it to https://github.com/j3-fortran/fortran_proposals. With Gary\u0026rsquo;s help we have convinced several members of the U.S. Standards Committee to participate. The first week was critical, here are all the people that contributed to issues:\nAfter two weeks the most active 12 people:\nAnd by the end of February 2020 the most active 12 people:\nThis succession of stats show how the community was growing in the initial stages.\nI have spent tremendous effort in this timeframe to nurture this interest and to get people involved. I replied to every new contributor, and directed the discussion to be the most productive. I haven\u0026rsquo;t done much work on LFortran in this timeframe, but I felt this was the opportunity to build an active online Fortran community so I gave it my best.\nStdlib Milan suggested to start stdlib on November 28, 2019. We got a \u0026ldquo;blessing\u0026rdquo; for this effort from Steve Lionel. I asked Milan if he would be interested in leading this effort, he accepted. Things were moving fast!\nMilan organized the community around stdlib. We started a GitHub organization fortran-lang. I spent a lot of effort providing initial code and discussions in stdlib also. Here are the first 12 contributors in the first month and a half:\nMy approach with these large initial contributions is to provide enough initial momentum so that the snowball eventually starts rolling on its own, whether or not I contribute. And we succeeded, for example in October and November 2020 my own contributions are negligible and others have contributed an order of magnitude more (the first 12 people):\nIt works!\nFortran Package Manager As soon as we started developing LFortran, it became obvious that we will need a package ecosystem, so I created an issue for it on May 22, 2019 with what I believe the goals should be. On October 27, 2019 when we had our J3 Incubator Repository, I created an issue there with the same goals. There it generated a lot more discussion. On December 23, 2019 Dominik Gronkiewicz opened a new issue at stdlib to try to create an experimental packaging ecosystem. It got even more interest. More crucially, Milan wrote \u0026ldquo;I\u0026rsquo;m in\u0026rdquo;, which was enough momentum that we created a new repository for the Fortran Package Manager (fpm).\nI initially created a prototype in Rust, but I got stuck on a weird issue on macOS where a CI test would randomly fail. I spent dozens and dozens of hours on that, asked at the Rust Discourse, the Rust community has been very helpful. Eventually we figured out a workaround, but it took a lot of time and effort. I did not have to spend the time on this, but I am the kind of guy who expects things to work robustly on all platforms at all times and I cannot sleep well knowing that a Pull Request CI tests can randomly fail at any time. As a consequence of this, we did not get very far. But it confirmed my suspicion that Cargo is the best package manager I know of, the state-of-the-art. And that we want to implement something similar for Fortran.\nJanuary 2020 turned into February and there was a Standards Committee Meeting. One of the highlights for me was that Brad Richardson agreed to join us to lead the fpm effort. He already had a prototype in Haskell, so we made a gentleman\u0026rsquo;s agreement that we will switch over to his code to finish the prototype, and then we will re-evaluate.\nMilan\u0026rsquo;s and Brad\u0026rsquo;s posts have more details what happened next.\nBy now, I consider fpm the most successful and exciting project that came out of our Fortran-lang efforts so far. The production version is written in Fortran, it can build itself, and there is a growing ecosystem of packages. It is modeled by Cargo, the Rust package manager. We have a solid foundation to build upon in the coming years.\nFortran Website On Jan 17 2020 I suggested to create a website for Fortran. We got it up and running in April 2020 at fortran-lang.org, a domain that Milan has bought some time ago, waiting for the right time to use it. By October 25, the webpage was second in all search engines, except Google where it was 9th. By December 2, it was 4th in Google: Wikipedia now uses our webpage as the Fortran page and our logo. I really like the logo, it was Jacob Williams\u0026rsquo;s and Milan\u0026rsquo;s idea, see Milan\u0026rsquo;s blog post for more details.\nWe have achieved something incredible: in less than a year we managed to get the Fortran website to top of all major search engines.\nFortran Standards Committee In April 2020 I announced a run for WG5 Convenor (International Fortran Committee Chair), you can read my platform and motivation in the announcement post. I did not get the position, Steve Lionel was selected as the chair. Later I ran for a J3 chair position, also did not get it, Brian Friesen was selected for it. I maintain excellent, professional relationships with both, as well as the rest of the committee. The committee as well as Steve and Brian all want to see Fortran improve. I wish them luck and I\u0026rsquo;ve helped them and the committee, as my time and energy allows.\nAs a highlight of the February 2021 meeting, we got Milan to officially join the committee also, thanks to Gary Klimowicz.\nFortranCon 2020 Another lucky coincidence was that Tiziano Mueller and team organized the very first FortranCon 2020. We helped announce at fortran-lang.org also.\nThe organizers have done a great job. The conference attracted a very enthusiastic group of people. It reminded me the early SciPy conferences. The community was small, and everybody understood we had to help each other. FortranCon felt just like that. Even though it was remote only, the atmosphere was excellent. A lot of good discussions happened.\nCommunity There have been many people instrumental to the success of the above efforts. You can find their names when you browse the community statistics at the bottom of: https://fortran-lang.org/community/.\nThe first and foremost that I want to thank to is Milan. I believe my first text message to Milan was on January 5 2020, setting up time to brainstorm fpm. We have exchanged hundreds of messages since then and dozens of phone calls. If it was not for him, the efforts above such as stdlib, fpm, website, etc. would simply not happen. He took the initiative to lead many of these efforts and provided me with the motivation to continue. He had many excellent, complementary ideas, such as starting stdlib, the Fortran monthly calls, having a Discourse in addition to a mailinglist (the Discourse proved a lot more popular), having a dedicated Twitter account for Fortran, and many other. The contributor statistics I showed above are based on comments only, so they do not tell the full story.\nI want to thank Jeremie Vandenplas, Brad Richardson, Laurence Kedward, Peter Klausler, Gary Klimowicz, Ivan Pribec, urbanjost, FortranFan, Zaak Beekman, Sebastian Ehlert, William Clodius, Arjen Markus, Bálint Aradi, Steve Lionel, Jacob Williams, Dominik Gronkiewicz, Michael Hirsch, Marshall Ward, Chris MacMackin, septcolor, Neil Carlson and the rest of over 100 people who have contributed. It has been a pleasure to work with you all on reviving Fortran.\nConclusions We have finished the first chapter on our long journy of resurrecting Fortran. If I stop contributing today, I don\u0026rsquo;t believe these efforts will die. From this perspective I have succeeded. As documented above, it was highly non-trivial and a tremendous amount of work to bootstrap this initial Fortran community just to get to this point.\nWe still have a long way to go, but the focus of our work has shifted. Now our job is to grow and leverage the community to make fast enough progress on the core efforts such as fpm, stdlib and other tooling. We have the platform, infrastructure and the community to do it.\nI personally want to focus more on LFortran to make it succeed. Things look very promising, just in the last month we got 4 people contributing code to the compiler. I feel it is the last missing piece of the puzzle. Together with the new Flang, GFortran, commercial Fortran compilers (https://fortran-lang.org/compilers/) and collaboration with the standards committee I believe we will eventually succeed in making Fortran a viable language for high performance numerical computing going forward.\nDiscussions:\nTwitter Hacker News ","permalink":"https://ondrejcertik.com/blog/2021/03/resurrecting-fortran/","tags":["Fortran"],"title":"Resurrecting Fortran"},{"categories":null,"contents":"I decided to run for WG5 Convenor (International Fortran Committee Chair). I have written up a Platform document that explains why I am running, the issues as I see them, and how I propose to work with the Committee to address them (see also my CV). I have developed this platform with many Fortran community members, both inside and outside of the Fortran Committee. Furthermore, I have also posted the platform as a GitHub repository and I encourage anyone to open issues and Pull Requests (PRs) to discuss and further refine the platform.\nThe platform details what I would do as WG5 Convenor, if selected. My main goal is to focus on the original mission of Fortran that should still be the mission today: enable scientists, engineers, and other domain experts to write programs that naturally express the mathematics and algorithms employed, are portable across HPC systems, remain viable over decades of use, and extract a high percentage of performance from the underlying hardware.\nTo achieve this mission, the Committee should increase engagement with Fortran users in national labs, academia, industry, and elsewhere, understand the underlying reasons why many efforts are moving away from Fortran, and address them fully and expeditiously. The most common feedback I hear from Fortran users is that they want their code to run on modern hardware, which as outlined above, has always been a Fortran mission. Unfortunately, in this mission, we currently are falling short. For example, Fortran does not currently have a clear path forward to fully leverage GPUs which are part of most HPC systems today; and I have personally seen several projects moving away from Fortran for this very reason. Another example is the relative lack of library ecosystem and developer tools around Fortran compared to other modern languages.\nIn order to start delivering on our mission again, we need to substantially improve both the compilers and tooling around Fortran. And we need to work with the wider user community to deliver on the most used/requested capabilities. Given my background in large-scale scientific applications, software development, and tooling, I believe I can bring this user perspective to the Committee. I hope to have the opportunity as the WG5 Convenor to make this a reality by working with the Committee to implement the items in the platform; cultivating a climate of openness, listening, and cooperation to accomplish common goals; and facilitating collaboration with the wider Fortran community. In so doing, we will lay the groundwork to address the major challenges (such as GPU support) before us, which can only be addressed fully and expeditiously with an efficient and productive collaboration of Fortran users, compiler vendors, and Committee.\n","permalink":"https://ondrejcertik.com/blog/2020/04/running-for-wg5-convenor-announcement/","tags":["Fortran","WG5","Convenor"],"title":"Running for WG5 Convenor Announcement"},{"categories":null,"contents":"My webpage is generated using Hugo and it uses the hugo-resume template.\nI just enabled a blog section here, so that I can post blog posts here from now on. My old blog is still hosted at https://ondrejcertik.blogspot.com/, but I do not update it anymore.\n","permalink":"https://ondrejcertik.com/blog/2020/04/enabling-blog-at-my-webpage/","tags":["hugo","blog"],"title":"Enabling Blog At My Webpage"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"https://ondrejcertik.com/search/","tags":null,"title":"Search Results"}]